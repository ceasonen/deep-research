\# ðŸ” AutoSearch AI - æ™ºèƒ½è”ç½‘æœç´¢å¢žå¼ºå·¥å…· ## å®Œæ•´å¼€æºé¡¹ç›®æ–¹æ¡ˆï¼ˆè¶…è¯¦ç»†ç‰ˆï¼‰ --- ## ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®å®šä½ä¸Žå¸‚åœºåˆ†æž ### 1.1 ä¸ºä»€ä¹ˆè¿™ä¸ªé¡¹ç›®æœ‰çˆ†æ¬¾æ½œåŠ› å½“å‰å¸‚åœºä¸Šå­˜åœ¨ä¸€ä¸ªæ˜Žæ˜¾çš„éœ€æ±‚ç¼ºå£ï¼š **ç”¨æˆ·ç—›ç‚¹ï¼š** - ChatGPTç­‰å¤§æ¨¡åž‹çš„çŸ¥è¯†æœ‰æˆªæ­¢æ—¥æœŸï¼Œæ— æ³•èŽ·å–å®žæ—¶ä¿¡æ¯ - Perplexity AI æ˜¯é—­æºå•†ä¸šäº§å“ï¼Œå…è´¹é¢åº¦æœ‰é™ - çŽ°æœ‰çš„å¼€æºè”ç½‘æœç´¢æ–¹æ¡ˆï¼ˆå¦‚ Perplexicaã€SearchGPT ç­‰ï¼‰è¦ä¹ˆé…ç½®å¤æ‚ï¼Œè¦ä¹ˆä¾èµ–ç‰¹å®š API æˆæœ¬é«˜ - å¼€å‘è€…å’Œæ™®é€šç”¨æˆ·éƒ½éœ€è¦ä¸€ä¸ª**è½»é‡ã€å…è´¹ã€ä¸€é”®éƒ¨ç½²**çš„è”ç½‘æœç´¢å·¥å…· **å¸‚åœºéªŒè¯ï¼ˆé€šè¿‡GitHubè¶‹åŠ¿åˆ†æžï¼‰ï¼š** - Perplexicaï¼ˆå¼€æº Perplexity æ›¿ä»£å“ï¼‰ï¼šèŽ·å¾—äº† 10k+ stars - MindSearchï¼ˆä¸­ç§‘é™¢çš„è”ç½‘æœç´¢ï¼‰ï¼šçŸ­æ—¶é—´å†… 3k+ stars - phidata/agent-search ç±»é¡¹ç›®ï¼šæŒç»­å¢žé•¿ **æˆ‘ä»¬çš„å·®å¼‚åŒ–å®šä½ï¼š** | ç»´åº¦ | Perplexica | MindSearch | **AutoSearch AIï¼ˆæˆ‘ä»¬çš„ï¼‰** | |------|-----------|------------|--------------------------| | éƒ¨ç½²éš¾åº¦ | ä¸­ç­‰ | è¾ƒé«˜ | **æžä½Žï¼ˆä¸€æ¡å‘½ä»¤ï¼‰** | | LLM ä¾èµ– | éœ€è¦å¤§æ¨¡åž‹API | éœ€è¦å¤§æ¨¡åž‹API | **æ”¯æŒæœ¬åœ°å°æ¨¡åž‹ + API åŒæ¨¡å¼** | | æœç´¢å¼•æ“Ž | SearxNGï¼ˆéœ€éƒ¨ç½²ï¼‰ | Google APIï¼ˆéœ€ä»˜è´¹ï¼‰ | **å¤šå¼•æ“Žå…è´¹èšåˆ** | | ç‰¹è‰²åŠŸèƒ½ | åŸºç¡€é—®ç­” | å¤šæ­¥æŽ¨ç† | **å®žæ—¶æœç´¢ + æ‘˜è¦ + å¼•ç”¨è¿½è¸ª + çŸ¥è¯†å›¾è°±å¯è§†åŒ–** | | æƒé‡/æ¨¡åž‹ | æ—  | æ—  | **å¯é€‰çš„è½»é‡çº§ reranker æ¨¡åž‹ï¼ˆ~50MBï¼‰** | ### 1.2 é¡¹ç›®å‘½åä¸Žå“ç‰Œ **é¡¹ç›®åï¼š`AutoSearch AI`** - å¤‡é€‰åï¼š`OpenSearch-AI` / `SearchFlow` / `RealTime-AI` - Slogan: *"Your AI-powered real-time search engine, open source and self-hosted."* ### 1.3 æ ¸å¿ƒç‰¹æ€§ï¼ˆå–ç‚¹ï¼‰ 1. **ðŸš€ ä¸€é”®éƒ¨ç½²**ï¼š`pip install autosearch-ai && autosearch serve` 2. **ðŸ”„ å¤šæœç´¢å¼•æ“Žèšåˆ**ï¼šGoogleã€Bingã€DuckDuckGoã€Brave å…è´¹æœç´¢ 3. **ðŸ§  æ™ºèƒ½æ‘˜è¦**ï¼šæ”¯æŒ OpenAI / Ollama æœ¬åœ°æ¨¡åž‹ / ä»»ä½• OpenAI å…¼å®¹ API 4. **ðŸ“Š å¼•ç”¨è¿½è¸ª**ï¼šæ¯å¥è¯æ ‡æ³¨æ¥æºï¼Œå¯éªŒè¯ 5. **ðŸŽ¯ è½»é‡çº§ Reranker**ï¼šå¯é€‰çš„ ~50MB å°æ¨¡åž‹ï¼Œå¯¹æœç´¢ç»“æžœé‡æŽ’åºï¼Œæ˜¾è‘—æå‡è´¨é‡ 6. **ðŸ–¥ï¸ ç²¾ç¾Ž Web UI**ï¼šçŽ°ä»£åŒ–ç•Œé¢ï¼Œæ”¯æŒæš—è‰²æ¨¡å¼ 7. **ðŸ”Œ API ä¼˜å…ˆ**ï¼šæä¾› RESTful APIï¼Œå¯é›†æˆåˆ°ä»»ä½•é¡¹ç›® 8. **ðŸ“± å“åº”å¼è®¾è®¡**ï¼šæ‰‹æœºç«¯å®Œç¾Žé€‚é… --- ## ç¬¬äºŒéƒ¨åˆ†ï¼šç³»ç»Ÿæž¶æž„è®¾è®¡ ### 2.1 æ•´ä½“æž¶æž„å›¾ ``` â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                        Frontend (React/Next.js)              â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚  â”‚ Search UI â”‚  â”‚ Result Panel â”‚  â”‚ Knowledge Graph View   â”‚ â”‚ â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚        â”‚               â”‚                      â”‚              â”‚ â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚                        â”‚ WebSocket / REST API                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   Backend (FastAPI)                           â”‚ â”‚                        â”‚                                     â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚  â”‚              Query Processor                     â”‚         â”‚ â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚  â”‚  â”‚ Query    â”‚ â”‚ Intent    â”‚ â”‚ Sub-query      â”‚ â”‚         â”‚ â”‚  â”‚  â”‚ Parser   â”‚ â”‚ Classifierâ”‚ â”‚ Decomposer     â”‚ â”‚         â”‚ â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚                        â”‚                                     â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚  â”‚            Search Engine Aggregator              â”‚         â”‚ â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚  â”‚  â”‚ Google â”‚ â”‚ Bing â”‚ â”‚DuckDuckGoâ”‚ â”‚ Brave   â”‚ â”‚         â”‚ â”‚  â”‚  â”‚Scraper â”‚ â”‚ API  â”‚ â”‚  API     â”‚ â”‚ Search  â”‚ â”‚         â”‚ â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚                        â”‚                                     â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚  â”‚           Content Processor Pipeline             â”‚         â”‚ â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚  â”‚  â”‚ Web Page â”‚ â”‚ Content   â”‚ â”‚ Reranker       â”‚ â”‚         â”‚ â”‚  â”‚  â”‚ Fetcher  â”‚ â”‚ Extractor â”‚ â”‚ (Small Model)  â”‚ â”‚         â”‚ â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚                        â”‚                                     â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚  â”‚            LLM Synthesizer                       â”‚         â”‚ â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”‚  â”‚  â”‚ Prompt       â”‚  â”‚ Response Generator       â”‚ â”‚         â”‚ â”‚  â”‚  â”‚ Constructor  â”‚  â”‚ (Streaming + Citations)  â”‚ â”‚         â”‚ â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               External Services                              â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚  â”‚ OpenAI   â”‚ â”‚ Ollama     â”‚ â”‚ Any OpenAI-compatible API  â”‚ â”‚ â”‚  â”‚ API      â”‚ â”‚ (Local)    â”‚ â”‚ (Groq, Together, etc.)     â”‚ â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ``` ### 2.2 è¿è¡Œé€»è¾‘ï¼ˆæ ¸å¿ƒæµç¨‹ï¼‰ ``` ç”¨æˆ·è¾“å…¥æŸ¥è¯¢    â”‚    â–¼ Step 1: æŸ¥è¯¢åˆ†æžä¸Žåˆ†è§£    â”‚  - åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘ï¼ˆç®€å•é—®å€™ä¸éœ€è¦ï¼‰    â”‚  - å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢    â”‚  - æå–å…³é”®æœç´¢è¯    â”‚    â–¼ Step 2: å¹¶å‘æœç´¢    â”‚  - åŒæ—¶å‘å¤šä¸ªæœç´¢å¼•æ“Žå‘é€è¯·æ±‚    â”‚  - æ”¶é›†æœç´¢ç»“æžœï¼ˆæ ‡é¢˜ã€URLã€æ‘˜è¦ç‰‡æ®µï¼‰    â”‚  - åŽ»é‡åˆå¹¶    â”‚    â–¼ Step 3: ç½‘é¡µå†…å®¹æå–    â”‚  - å¹¶å‘æŠ“å– Top-N ç½‘é¡µ    â”‚  - æå–æ­£æ–‡å†…å®¹ï¼ˆåŽ»é™¤å¹¿å‘Šã€å¯¼èˆªç­‰å™ªå£°ï¼‰    â”‚  - æˆªæ–­åˆ°åˆé€‚é•¿åº¦    â”‚    â–¼ Step 4: ç»“æžœé‡æŽ’åºï¼ˆå¯é€‰ï¼Œä½¿ç”¨å°æ¨¡åž‹ï¼‰    â”‚  - ä½¿ç”¨è½»é‡çº§ cross-encoder æ¨¡åž‹    â”‚  - å¯¹ query-document ç›¸å…³æ€§è¯„åˆ†    â”‚  - é€‰æ‹©æœ€ç›¸å…³çš„ Top-K ç‰‡æ®µ    â”‚    â–¼ Step 5: LLM ç»¼åˆå›žç­”    â”‚  - æž„å»ºåŒ…å«æœç´¢ç»“æžœçš„ prompt    â”‚  - æµå¼ç”Ÿæˆå›žç­”    â”‚  - æ ‡æ³¨å¼•ç”¨æ¥æº [1][2][3]    â”‚    â–¼ Step 6: ç»“æžœå‘ˆçŽ°    - Markdown æ ¼å¼çš„å›žç­”    - å¯ç‚¹å‡»çš„å¼•ç”¨é“¾æŽ¥    - ç›¸å…³æœç´¢å»ºè®® ``` ### 2.3 æŠ€æœ¯æ ˆé€‰æ‹© | å±‚æ¬¡ | æŠ€æœ¯ | ç†ç”± | |------|------|------| | åŽç«¯ | Python + FastAPI | å¼‚æ­¥é«˜æ€§èƒ½ï¼Œç”Ÿæ€ä¸°å¯Œ | | å‰ç«¯ | Next.js 14 (App Router) | SSR/SSGï¼Œæµå¼æ¸²æŸ“å¥½ | | æœç´¢ | DuckDuckGoï¼ˆé»˜è®¤å…è´¹ï¼‰+ å¯é€‰ Google/Bing | é›¶æˆæœ¬å¯åŠ¨ | | ç½‘é¡µè§£æž | Trafilatura + BeautifulSoup | æœ€ä½³æ­£æ–‡æå– | | Reranker | `cross-encoder/ms-marco-MiniLM-L-6-v2`ï¼ˆ~80MBï¼‰ | å°æƒé‡é«˜æ•ˆæžœ | | LLM å¯¹æŽ¥ | OpenAI SDKï¼ˆå…¼å®¹ Ollama/Groq/ä»»æ„APIï¼‰ | ç»Ÿä¸€æŽ¥å£ | | æµå¼ä¼ è¾“ | Server-Sent Events (SSE) | ç®€å•å¯é  | | éƒ¨ç½² | Docker + Docker Compose | ä¸€é”®éƒ¨ç½² | --- ## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´ä»£ç å®žçŽ° ### 3.1 é¡¹ç›®ç›®å½•ç»“æž„ ``` autosearch-ai/ â”œâ”€â”€ README.md â”œâ”€â”€ LICENSE (MIT) â”œâ”€â”€ pyproject.toml â”œâ”€â”€ Dockerfile â”œâ”€â”€ docker-compose.yml â”œâ”€â”€ .env.example â”œâ”€â”€ .github/ â”‚   â”œâ”€â”€ workflows/ â”‚   â”‚   â”œâ”€â”€ ci.yml â”‚   â”‚   â””â”€â”€ release.yml â”‚   â””â”€â”€ ISSUE_TEMPLATE/ â”‚       â”œâ”€â”€ bug_report.md â”‚       â””â”€â”€ feature_request.md â”œâ”€â”€ docs/ â”‚   â”œâ”€â”€ architecture.md â”‚   â”œâ”€â”€ api-reference.md â”‚   â”œâ”€â”€ deployment.md â”‚   â””â”€â”€ contributing.md â”œâ”€â”€ backend/ â”‚   â”œâ”€â”€ __init__.py â”‚   â”œâ”€â”€ main.py                    # FastAPI å…¥å£ â”‚   â”œâ”€â”€ config.py                  # é…ç½®ç®¡ç† â”‚   â”œâ”€â”€ models/ â”‚   â”‚   â”œâ”€â”€ __init__.py â”‚   â”‚   â”œâ”€â”€ schemas.py             # Pydantic æ¨¡åž‹ â”‚   â”‚   â””â”€â”€ reranker.py            # Reranker å°æ¨¡åž‹ â”‚   â”œâ”€â”€ search/ â”‚   â”‚   â”œâ”€â”€ __init__.py â”‚   â”‚   â”œâ”€â”€ base.py                # æœç´¢å¼•æ“ŽåŸºç±» â”‚   â”‚   â”œâ”€â”€ duckduckgo.py          # DuckDuckGo æœç´¢ â”‚   â”‚   â”œâ”€â”€ google.py              # Google æœç´¢ â”‚   â”‚   â”œâ”€â”€ bing.py                # Bing æœç´¢ â”‚   â”‚   â”œâ”€â”€ brave.py               # Brave æœç´¢ â”‚   â”‚   â””â”€â”€ aggregator.py          # æœç´¢èšåˆå™¨ â”‚   â”œâ”€â”€ content/ â”‚   â”‚   â”œâ”€â”€ __init__.py â”‚   â”‚   â”œâ”€â”€ fetcher.py             # ç½‘é¡µæŠ“å– â”‚   â”‚   â””â”€â”€ extractor.py           # æ­£æ–‡æå– â”‚   â”œâ”€â”€ llm/ â”‚   â”‚   â”œâ”€â”€ __init__.py â”‚   â”‚   â”œâ”€â”€ client.py              # LLM å®¢æˆ·ç«¯ â”‚   â”‚   â”œâ”€â”€ prompts.py             # Prompt æ¨¡æ¿ â”‚   â”‚   â””â”€â”€ synthesizer.py         # å›žç­”åˆæˆ â”‚   â”œâ”€â”€ pipeline/ â”‚   â”‚   â”œâ”€â”€ __init__.py â”‚   â”‚   â””â”€â”€ search_pipeline.py     # æ ¸å¿ƒæœç´¢ç®¡çº¿ â”‚   â””â”€â”€ utils/ â”‚       â”œâ”€â”€ __init__.py â”‚       â”œâ”€â”€ logger.py â”‚       â””â”€â”€ cache.py               # ç®€å•ç¼“å­˜ â”œâ”€â”€ frontend/ â”‚   â”œâ”€â”€ package.json â”‚   â”œâ”€â”€ next.config.js â”‚   â”œâ”€â”€ tailwind.config.js â”‚   â”œâ”€â”€ tsconfig.json â”‚   â”œâ”€â”€ public/ â”‚   â”‚   â”œâ”€â”€ favicon.ico â”‚   â”‚   â””â”€â”€ logo.svg â”‚   â”œâ”€â”€ src/ â”‚   â”‚   â”œâ”€â”€ app/ â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx           # ä¸»é¡µ â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css â”‚   â”‚   â”‚   â””â”€â”€ api/               # API routes (å¯é€‰ä»£ç†) â”‚   â”‚   â”œâ”€â”€ components/ â”‚   â”‚   â”‚   â”œâ”€â”€ SearchBar.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ SearchResults.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ SourceCard.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ AnswerPanel.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ LoadingAnimation.tsx â”‚   â”‚   â”‚   â”œâ”€â”€ ThemeToggle.tsx â”‚   â”‚   â”‚   â””â”€â”€ Header.tsx â”‚   â”‚   â”œâ”€â”€ hooks/ â”‚   â”‚   â”‚   â”œâ”€â”€ useSearch.ts â”‚   â”‚   â”‚   â””â”€â”€ useSSE.ts â”‚   â”‚   â”œâ”€â”€ lib/ â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts â”‚   â”‚   â”‚   â””â”€â”€ utils.ts â”‚   â”‚   â””â”€â”€ types/ â”‚   â”‚       â””â”€â”€ index.ts â”‚   â””â”€â”€ public/ â”‚       â””â”€â”€ demo.gif               # æ¼”ç¤º GIF â””â”€â”€ tests/    â”œâ”€â”€ __init__.py    â”œâ”€â”€ test_search.py    â”œâ”€â”€ test_content.py    â”œâ”€â”€ test_pipeline.py    â””â”€â”€ test_api.py ``` ### 3.2 åŽç«¯æ ¸å¿ƒä»£ç  #### `backend/config.py` - é…ç½®ç®¡ç† ```python """ AutoSearch AI - Configuration Management Supports environment variables, .env files, and sensible defaults. """ from pydantic_settings import BaseSettings from typing import Optional, List, Literal from functools import lru_cache  class Settings(BaseSettings):    """Application settings with environment variable support."""        # === App Settings ===    app_name: str = "AutoSearch AI"    app_version: str = "0.1.0"    debug: bool = False    host: str = "0.0.0.0"    port: int = 8000        # === LLM Settings ===    llm_provider: Literal["openai", "ollama", "groq", "together", "custom"] = "openai"    llm_api_key: Optional[str] = None    llm_base_url: Optional[str] = None  # For Ollama: http://localhost:11434/v1    llm_model: str = "gpt-4o-mini"    llm_temperature: float = 0.1    llm_max_tokens: int = 4096        # === Search Settings ===    search_engines: List[str] = ["duckduckgo"]  # Available: duckduckgo, google, bing, brave    search_max_results: int = 10    search_region: str = "wt-wt"  # Worldwide    search_language: str = "en"        # === Content Settings ===    content_max_pages: int = 6      # Max pages to fetch content from    content_max_length: int = 3000  # Max chars per page content    content_timeout: int = 10       # Seconds        # === Reranker Settings ===    reranker_enabled: bool = True    reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"    reranker_top_k: int = 5        # === API Keys for Premium Search ===    google_api_key: Optional[str] = None    google_cx_id: Optional[str] = None    bing_api_key: Optional[str] = None    brave_api_key: Optional[str] = None        # === Cache Settings ===    cache_enabled: bool = True    cache_ttl: int = 3600  # 1 hour    cache_max_size: int = 1000        # === CORS ===    cors_origins: List[str] = ["http://localhost:3000", "http://localhost:8000"]        class Config:        env_file = ".env"        env_file_encoding = "utf-8"        case_sensitive = False  @lru_cache() def get_settings() -> Settings:    return Settings() ``` #### `backend/models/schemas.py` - æ•°æ®æ¨¡åž‹ ```python """ Pydantic models for request/response schemas. """ from pydantic import BaseModel, Field from typing import List, Optional, Dict, Any from datetime import datetime from enum import Enum  class SearchMode(str, Enum):    QUICK = "quick"        # Fast, fewer sources    DEEP = "deep"          # Thorough, more sources    ACADEMIC = "academic"  # Focus on academic/reliable sources  class SearchRequest(BaseModel):    """User search query request."""    query: str = Field(..., min_length=1, max_length=1000, description="Search query")    mode: SearchMode = Field(default=SearchMode.QUICK, description="Search mode")    max_sources: int = Field(default=6, ge=1, le=20, description="Max sources to use")    language: str = Field(default="en", description="Response language")    stream: bool = Field(default=True, description="Enable streaming response")        class Config:        json_schema_extra = {            "example": {                "query": "What are the latest developments in AI agents in 2024?",                "mode": "quick",                "max_sources": 6,                "language": "en",                "stream": True            }        }  class SearchSource(BaseModel):    """A single search result source."""    title: str    url: str    snippet: str    content: Optional[str] = None    relevance_score: Optional[float] = None    source_engine: str = "unknown"    favicon_url: Optional[str] = None    published_date: Optional[str] = None  class SearchResponse(BaseModel):    """Complete search response."""    query: str    answer: str    sources: List[SearchSource]    related_queries: List[str] = []    search_time: float  # seconds    model_used: str    timestamp: datetime = Field(default_factory=datetime.utcnow)     class StreamEvent(BaseModel):    """Server-Sent Event data structure."""    event: str  # "sources", "answer_start", "answer_chunk", "answer_end", "related", "error"    data: Any  class HealthResponse(BaseModel):    """Health check response."""    status: str = "healthy"    version: str    llm_connected: bool    reranker_loaded: bool    search_engines: List[str] ``` #### `backend/models/reranker.py` - è½»é‡çº§é‡æŽ’åºæ¨¡åž‹ ```python """ Lightweight reranker model for improving search result relevance. Uses a small cross-encoder model (~80MB) that runs on CPU. """ import logging from typing import List, Tuple, Optional from backend.config import get_settings logger = logging.getLogger(__name__) # Global model instance (lazy loaded) _reranker_model = None _reranker_tokenizer = None  class Reranker:    """    Cross-encoder reranker using a small pretrained model.    Model: cross-encoder/ms-marco-MiniLM-L-6-v2 (~80MB)    This significantly improves search quality by reranking results    based on query-document relevance.    """        def __init__(self):        self.settings = get_settings()        self.model = None        self.is_loaded = False            def load(self) -> bool:        """Load the reranker model. Returns True if successful."""        if not self.settings.reranker_enabled:            logger.info("Reranker is disabled in settings")            return False                    try:            from sentence_transformers import CrossEncoder                        logger.info(f"Loading reranker model: {self.settings.reranker_model}")            self.model = CrossEncoder(                self.settings.reranker_model,                max_length=512,                device="cpu"  # Small enough for CPU            )            self.is_loaded = True            logger.info("Reranker model loaded successfully")            return True                    except ImportError:            logger.warning(                "sentence-transformers not installed. "                "Install with: pip install sentence-transformers"            )            return False        except Exception as e:            logger.error(f"Failed to load reranker model: {e}")            return False        def rerank(        self,         query: str,         documents: List[dict],        text_key: str = "content",        top_k: Optional[int] = None    ) -> List[dict]:        """        Rerank documents by relevance to query.                Args:            query: The search query            documents: List of document dicts            text_key: Key in document dict containing text to rank            top_k: Number of top results to return                    Returns:            Reranked list of documents with added 'relevance_score'        """        if not self.is_loaded or not documents:            return documents                    top_k = top_k or self.settings.reranker_top_k                try:            # Prepare query-document pairs            pairs = []            valid_indices = []                        for i, doc in enumerate(documents):                text = doc.get(text_key) or doc.get("snippet", "")                if text.strip():                    # Truncate to avoid exceeding max_length                    pairs.append([query, text[:1000]])                    valid_indices.append(i)                        if not pairs:                return documents[:top_k]                        # Get relevance scores            scores = self.model.predict(pairs)                        # Attach scores and sort            scored_docs = []            score_idx = 0            for i, doc in enumerate(documents):                if i in valid_indices:                    doc_copy = doc.copy()                    doc_copy["relevance_score"] = float(scores[score_idx])                    scored_docs.append(doc_copy)                    score_idx += 1                else:                    doc_copy = doc.copy()                    doc_copy["relevance_score"] = -999.0                    scored_docs.append(doc_copy)                        # Sort by relevance score (descending)            scored_docs.sort(key=lambda x: x["relevance_score"], reverse=True)                        return scored_docs[:top_k]                    except Exception as e:            logger.error(f"Reranking failed: {e}")            return documents[:top_k]  # Singleton instance _reranker_instance: Optional[Reranker] = None  def get_reranker() -> Reranker:    """Get or create the reranker singleton."""    global _reranker_instance    if _reranker_instance is None:        _reranker_instance = Reranker()        _reranker_instance.load()    return _reranker_instance ``` #### `backend/search/base.py` - æœç´¢å¼•æ“ŽåŸºç±» ```python """ Base class for all search engine implementations. """ from abc import ABC, abstractmethod from typing import List from backend.models.schemas import SearchSource import logging logger = logging.getLogger(__name__)  class BaseSearchEngine(ABC):    """Abstract base class for search engines."""        name: str = "base"        @abstractmethod    async def search(self, query: str, max_results: int = 10, **kwargs) -> List[SearchSource]:        """        Perform a search and return results.                Args:            query: Search query string            max_results: Maximum number of results to return                    Returns:            List of SearchSource objects        """        pass        def _clean_snippet(self, snippet: str) -> str:        """Clean up a search result snippet."""        if not snippet:            return ""        # Remove excessive whitespace        import re        snippet = re.sub(r'\s+', ' ', snippet).strip()        return snippet[:500]  # Limit length ``` #### `backend/search/duckduckgo.py` - DuckDuckGo æœç´¢ï¼ˆå…è´¹ï¼Œæ ¸å¿ƒï¼‰ ```python """ DuckDuckGo search engine - Free, no API key required. This is the default search engine. """ import asyncio import logging from typing import List from backend.search.base import BaseSearchEngine from backend.models.schemas import SearchSource logger = logging.getLogger(__name__)  class DuckDuckGoSearch(BaseSearchEngine):    """DuckDuckGo search using the duckduckgo-search library."""        name = "duckduckgo"        async def search(self, query: str, max_results: int = 10, **kwargs) -> List[SearchSource]:        """Search DuckDuckGo and return results."""        try:            from duckduckgo_search import DDGS                        # Run synchronous search in a thread pool            loop = asyncio.get_event_loop()            results = await loop.run_in_executor(                None,                 self._sync_search,                 query,                 max_results,                kwargs.get("region", "wt-wt")            )            return results                    except Exception as e:            logger.error(f"DuckDuckGo search failed: {e}")            return []        def _sync_search(self, query: str, max_results: int, region: str) -> List[SearchSource]:        """Synchronous search wrapper."""        from duckduckgo_search import DDGS                sources = []        try:            with DDGS() as ddgs:                results = list(ddgs.text(                    keywords=query,                    region=region,                    safesearch="moderate",                    max_results=max_results                ))                                for r in results:                    sources.append(SearchSource(                        title=r.get("title", ""),                        url=r.get("href", r.get("link", "")),                        snippet=self._clean_snippet(r.get("body", "")),                        source_engine=self.name,                    ))        except Exception as e:            logger.error(f"DuckDuckGo sync search error: {e}")                return sources ``` #### `backend/search/google.py` - Google æœç´¢ ```python """ Google Custom Search API integration. Requires API key and Custom Search Engine ID. """ import aiohttp import logging from typing import List, Optional from backend.search.base import BaseSearchEngine from backend.models.schemas import SearchSource logger = logging.getLogger(__name__)  class GoogleSearch(BaseSearchEngine):    """Google Custom Search API."""        name = "google"    BASE_URL = "https://www.googleapis.com/customsearch/v1"        def __init__(self, api_key: Optional[str] = None, cx_id: Optional[str] = None):        self.api_key = api_key        self.cx_id = cx_id        async def search(self, query: str, max_results: int = 10, **kwargs) -> List[SearchSource]:        if not self.api_key or not self.cx_id:            logger.warning("Google API key or CX ID not configured")            return []                sources = []        try:            params = {                "key": self.api_key,                "cx": self.cx_id,                "q": query,                "num": min(max_results, 10),  # Google API limit            }                        async with aiohttp.ClientSession() as session:                async with session.get(self.BASE_URL, params=params, timeout=10) as resp:                    if resp.status == 200:                        data = await resp.json()                        for item in data.get("items", []):                            sources.append(SearchSource(                                title=item.get("title", ""),                                url=item.get("link", ""),                                snippet=self._clean_snippet(                                    item.get("snippet", "")                                ),                                source_engine=self.name,                            ))                    else:                        logger.error(f"Google search HTTP {resp.status}")                                except Exception as e:            logger.error(f"Google search failed: {e}")                return sources ``` #### `backend/search/aggregator.py` - æœç´¢èšåˆå™¨ ```python """ Search aggregator - combines results from multiple search engines. """ import asyncio import logging from typing import List, Dict from urllib.parse import urlparse from backend.search.base import BaseSearchEngine from backend.search.duckduckgo import DuckDuckGoSearch from backend.search.google import GoogleSearch from backend.models.schemas import SearchSource from backend.config import get_settings logger = logging.getLogger(__name__)  class SearchAggregator:    """    Aggregates search results from multiple engines.    Deduplicates by URL domain+path and merges results.    """        def __init__(self):        self.settings = get_settings()        self.engines: Dict[str, BaseSearchEngine] = {}        self._init_engines()        def _init_engines(self):        """Initialize configured search engines."""        engine_map = {            "duckduckgo": lambda: DuckDuckGoSearch(),            "google": lambda: GoogleSearch(                api_key=self.settings.google_api_key,                cx_id=self.settings.google_cx_id            ),        }                for engine_name in self.settings.search_engines:            if engine_name in engine_map:                self.engines[engine_name] = engine_map[engine_name]()                logger.info(f"Initialized search engine: {engine_name}")            else:                logger.warning(f"Unknown search engine: {engine_name}")                # Ensure at least DuckDuckGo is available        if not self.engines:            self.engines["duckduckgo"] = DuckDuckGoSearch()            logger.info("Fallback to DuckDuckGo search engine")        async def search(self, query: str, max_results: int = 10) -> List[SearchSource]:        """        Search across all configured engines concurrently.                Args:            query: Search query            max_results: Max total results after dedup                    Returns:            Deduplicated and merged search results        """        # Run all engines concurrently        tasks = [            engine.search(query, max_results=max_results)            for engine in self.engines.values()        ]                results = await asyncio.gather(*tasks, return_exceptions=True)                # Flatten and filter errors        all_sources: List[SearchSource] = []        for i, result in enumerate(results):            if isinstance(result, Exception):                engine_name = list(self.engines.keys())[i]                logger.error(f"Search engine {engine_name} failed: {result}")                continue            all_sources.extend(result)                # Deduplicate by URL        deduped = self._deduplicate(all_sources)                logger.info(f"Search returned {len(all_sources)} results, {len(deduped)} after dedup")                return deduped[:max_results]        def _deduplicate(self, sources: List[SearchSource]) -> List[SearchSource]:        """Remove duplicate sources based on URL domain+path."""        seen_urls = set()        unique_sources = []                for source in sources:            # Normalize URL for comparison            parsed = urlparse(source.url)            normalized = f"{parsed.netloc}{parsed.path}".rstrip("/").lower()                        if normalized not in seen_urls and source.url:                seen_urls.add(normalized)                unique_sources.append(source)                return unique_sources ``` #### `backend/content/fetcher.py` - ç½‘é¡µå†…å®¹æŠ“å– ```python """ Async web page fetcher with concurrency control and timeout handling. """ import asyncio import aiohttp import logging from typing import List, Optional, Dict logger = logging.getLogger(__name__) # Common user agent to avoid blocks USER_AGENT = (    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" )  class WebFetcher:    """Fetches web pages asynchronously with rate limiting."""        def __init__(self, timeout: int = 10, max_concurrent: int = 5):        self.timeout = aiohttp.ClientTimeout(total=timeout)        self.semaphore = asyncio.Semaphore(max_concurrent)        self.headers = {            "User-Agent": USER_AGENT,            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",            "Accept-Language": "en-US,en;q=0.5",        }        async def fetch_page(self, url: str) -> Optional[str]:        """        Fetch a single web page.                Args:            url: URL to fetch                    Returns:            HTML content string or None if failed        """        async with self.semaphore:            try:                async with aiohttp.ClientSession(                    timeout=self.timeout,                    headers=self.headers                ) as session:                    async with session.get(url, allow_redirects=True, ssl=False) as response:                        if response.status == 200:                            content_type = response.headers.get("content-type", "")                            if "text/html" in content_type or "text/" in content_type:                                html = await response.text(errors="ignore")                                return html                            else:                                logger.debug(f"Skipping non-HTML content: {content_type}")                                return None                        else:                            logger.debug(f"HTTP {response.status} for {url}")                            return None                                        except asyncio.TimeoutError:                logger.debug(f"Timeout fetching {url}")                return None            except Exception as e:                logger.debug(f"Error fetching {url}: {e}")                return None        async def fetch_pages(self, urls: List[str]) -> Dict[str, Optional[str]]:        """        Fetch multiple pages concurrently.                Args:            urls: List of URLs to fetch                    Returns:            Dict mapping URL -> HTML content (or None if failed)        """        tasks = [self.fetch_page(url) for url in urls]        results = await asyncio.gather(*tasks)        return dict(zip(urls, results)) ``` #### `backend/content/extractor.py` - æ­£æ–‡æå– ```python """ Content extractor - extracts main text content from HTML pages. Uses trafilatura for high-quality extraction. """ import logging from typing import Optional import re logger = logging.getLogger(__name__)  class ContentExtractor:    """Extracts clean text content from HTML."""        def __init__(self, max_length: int = 3000):        self.max_length = max_length        def extract(self, html: str, url: str = "") -> Optional[str]:        """        Extract main content from HTML.                Args:            html: Raw HTML string            url: Source URL (for context)                    Returns:            Extracted text content or None        """        if not html:            return None                # Try trafilatura first (best quality)        content = self._extract_trafilatura(html, url)                # Fallback to BeautifulSoup        if not content or len(content) < 50:            content = self._extract_beautifulsoup(html)                if content:            # Clean and truncate            content = self._clean_text(content)            content = content[:self.max_length]                    return content if content and len(content) >= 50 else None        def _extract_trafilatura(self, html: str, url: str = "") -> Optional[str]:        """Extract using trafilatura library."""        try:            import trafilatura            text = trafilatura.extract(                html,                url=url,                include_comments=False,                include_tables=True,                no_fallback=False,                favor_precision=True,            )            return text        except Exception as e:            logger.debug(f"Trafilatura extraction failed: {e}")            return None        def _extract_beautifulsoup(self, html: str) -> Optional[str]:        """Fallback extraction using BeautifulSoup."""        try:            from bs4 import BeautifulSoup                        soup = BeautifulSoup(html, "html.parser")                        # Remove unwanted elements            for tag in soup(["script", "style", "nav", "header", "footer",                            "aside", "iframe", "noscript", "form"]):                tag.decompose()                        # Try to find main content area            main_content = (                soup.find("main") or                 soup.find("article") or                 soup.find(class_=re.compile(r"content|article|post|entry", re.I)) or                soup.find("body")            )                        if main_content:                text = main_content.get_text(separator="\n", strip=True)                return text                        return None                    except Exception as e:            logger.debug(f"BeautifulSoup extraction failed: {e}")            return None        def _clean_text(self, text: str) -> str:        """Clean extracted text."""        # Remove excessive newlines        text = re.sub(r'\n{3,}', '\n\n', text)        # Remove excessive spaces        text = re.sub(r'[ \t]{3,}', ' ', text)        # Remove common junk patterns        text = re.sub(r'(Cookie|cookie|COOKIE).*?(policy|Policy|accept|Accept).*?\n', '', text)        return text.strip() ``` #### `backend/llm/prompts.py` - Prompt æ¨¡æ¿ ```python """ Prompt templates for LLM synthesis. Carefully crafted for accurate, cited responses. """ SYSTEM_PROMPT = """You are AutoSearch AI, an intelligent search assistant that provides accurate, well-sourced answers based on real-time web search results. ## Guidelines: 1. **Always cite sources** using [1], [2], etc. that correspond to the provided sources 2. **Be accurate** - only state information that is supported by the search results 3. **Be comprehensive** but concise - cover key points without unnecessary verbosity 4. **Acknowledge uncertainty** - if search results are insufficient, say so 5. **Use markdown formatting** for readability (headers, lists, bold, etc.) 6. **Respond in the same language as the user's query** 7. **Prioritize recency** - prefer information from more recent sources ## Response Format: - Start with a direct answer to the question - Provide supporting details with citations - End with a brief summary if the topic is complex"""  def build_search_prompt(query: str, sources: list) -> str:    """    Build the user prompt with search results context.        Args:        query: User's search query        sources: List of source dicts with 'title', 'url', 'content'            Returns:        Formatted prompt string    """    context_parts = []        for i, source in enumerate(sources, 1):        title = source.get("title", "Unknown")        url = source.get("url", "")        content = source.get("content") or source.get("snippet", "")                context_parts.append(            f"[{i}] **{title}**\n"            f"URL: {url}\n"            f"Content: {content}\n"        )        context = "\n---\n".join(context_parts)        prompt = f"""Based on the following search results, answer the user's question. ## Search Results: {context} ## User Question: {query} ## Instructions: - Cite sources using [1], [2], etc. - If the search results don't contain enough information to fully answer, acknowledge this - Provide a comprehensive yet concise answer"""        return prompt  QUERY_DECOMPOSE_PROMPT = """Analyze the following user query and determine if it needs to be decomposed into sub-queries for better search results. User Query: {query} Respond in JSON format: {{    "needs_search": true/false,    "search_queries": ["query1", "query2", ...],    "reasoning": "brief explanation" }} Rules: - Simple greetings or chitchat don't need search (needs_search: false) - Complex questions may need 2-3 sub-queries - Keep search queries concise and search-engine friendly - Maximum 3 sub-queries - Return at least 1 search query if needs_search is true"""  RELATED_QUERIES_PROMPT = """Based on the following question and answer, suggest 3 related follow-up questions that the user might want to explore. Question: {query} Answer summary: {answer_summary} Return exactly 3 related questions, one per line, without numbering or bullets.""" ``` #### `backend/llm/client.py` - LLM å®¢æˆ·ç«¯ ```python """ Unified LLM client supporting OpenAI, Ollama, Groq, and any OpenAI-compatible API. """ import logging from typing import AsyncGenerator, Optional from openai import AsyncOpenAI from backend.config import get_settings logger = logging.getLogger(__name__)  class LLMClient:    """    Unified LLM client using OpenAI SDK.    Supports any OpenAI-compatible API (Ollama, Groq, Together, etc.)    """        def __init__(self):        self.settings = get_settings()        self.client = self._create_client()        def _create_client(self) -> AsyncOpenAI:        """Create the appropriate OpenAI client based on config."""        provider = self.settings.llm_provider                if provider == "ollama":            base_url = self.settings.llm_base_url or "http://localhost:11434/v1"            return AsyncOpenAI(                base_url=base_url,                api_key="ollama"  # Ollama doesn't need a real key            )        elif provider == "groq":            return AsyncOpenAI(                base_url="https://api.groq.com/openai/v1",                api_key=self.settings.llm_api_key            )        elif provider == "together":            return AsyncOpenAI(                base_url="https://api.together.xyz/v1",                api_key=self.settings.llm_api_key            )        elif provider == "custom":            return AsyncOpenAI(                base_url=self.settings.llm_base_url,                api_key=self.settings.llm_api_key or "not-needed"            )        else:  # openai            return AsyncOpenAI(                api_key=self.settings.llm_api_key            )        async def generate(        self,        messages: list,        model: Optional[str] = None,        temperature: Optional[float] = None,        max_tokens: Optional[int] = None,    ) -> str:        """        Generate a complete response (non-streaming).        """        try:            response = await self.client.chat.completions.create(                model=model or self.settings.llm_model,                messages=messages,                temperature=temperature or self.settings.llm_temperature,                max_tokens=max_tokens or self.settings.llm_max_tokens,            )            return response.choices[0].message.content or ""        except Exception as e:            logger.error(f"LLM generation failed: {e}")            raise        async def generate_stream(        self,        messages: list,        model: Optional[str] = None,        temperature: Optional[float] = None,        max_tokens: Optional[int] = None,    ) -> AsyncGenerator[str, None]:        """        Generate a streaming response, yielding text chunks.        """        try:            stream = await self.client.chat.completions.create(                model=model or self.settings.llm_model,                messages=messages,                temperature=temperature or self.settings.llm_temperature,                max_tokens=max_tokens or self.settings.llm_max_tokens,                stream=True,            )                        async for chunk in stream:                if chunk.choices[0].delta.content:                    yield chunk.choices[0].delta.content                            except Exception as e:            logger.error(f"LLM streaming failed: {e}")            raise        async def check_connection(self) -> bool:        """Test if the LLM is reachable."""        try:            response = await self.client.chat.completions.create(                model=self.settings.llm_model,                messages=[{"role": "user", "content": "Hi"}],                max_tokens=5,            )            return True        except Exception as e:            logger.error(f"LLM connection check failed: {e}")            return False ``` #### `backend/pipeline/search_pipeline.py` - æ ¸å¿ƒæœç´¢ç®¡çº¿ ```python """ Core search pipeline - orchestrates the entire search, fetch, rerank, synthesize flow. This is the heart of AutoSearch AI. """ import asyncio import json import logging import time from typing import AsyncGenerator, List, Optional, Dict, Any from backend.config import get_settings from backend.search.aggregator import SearchAggregator from backend.content.fetcher import WebFetcher from backend.content.extractor import ContentExtractor from backend.models.reranker import get_reranker from backend.llm.client import LLMClient from backend.llm.prompts import (    SYSTEM_PROMPT,     build_search_prompt,     QUERY_DECOMPOSE_PROMPT,    RELATED_QUERIES_PROMPT ) from backend.models.schemas import SearchSource, SearchRequest, StreamEvent logger = logging.getLogger(__name__)  class SearchPipeline:    """    Orchestrates the complete search pipeline:    1. Query analysis    2. Web search    3. Content fetching    4. Reranking    5. LLM synthesis    6. Related queries generation    """        def __init__(self):        self.settings = get_settings()        self.aggregator = SearchAggregator()        self.fetcher = WebFetcher(timeout=self.settings.content_timeout)        self.extractor = ContentExtractor(max_length=self.settings.content_max_length)        self.reranker = get_reranker()        self.llm = LLMClient()        async def search_stream(self, request: SearchRequest) -> AsyncGenerator[str, None]:        """        Execute search pipeline with streaming output.        Yields Server-Sent Events (SSE) formatted strings.        """        start_time = time.time()                try:            # === Step 1: Search ===            yield self._sse_event("status", {"message": "Searching the web...", "step": 1})                        search_results = await self.aggregator.search(                query=request.query,                max_results=request.max_sources * 2  # Fetch more for reranking            )                        if not search_results:                yield self._sse_event("error", {"message": "No search results found"})                return                        # Send sources to frontend immediately            sources_data = [                {                    "title": s.title,                    "url": s.url,                    "snippet": s.snippet,                    "source_engine": s.source_engine,                }                for s in search_results[:request.max_sources]            ]            yield self._sse_event("sources", sources_data)                        # === Step 2: Fetch Content ===            yield self._sse_event("status", {"message": "Reading web pages...", "step": 2})                        urls_to_fetch = [s.url for s in search_results[:self.settings.content_max_pages]]            html_results = await self.fetcher.fetch_pages(urls_to_fetch)                        # Extract content            enriched_sources = []            for source in search_results:                source_dict = source.model_dump()                html = html_results.get(source.url)                if html:                    content = self.extractor.extract(html, source.url)                    if content:                        source_dict["content"] = content                enriched_sources.append(source_dict)                        # === Step 3: Rerank ===            if self.reranker.is_loaded:                yield self._sse_event("status", {"message": "Analyzing relevance...", "step": 3})                                enriched_sources = self.reranker.rerank(                    query=request.query,                    documents=enriched_sources,                    text_key="content",                    top_k=request.max_sources                )            else:                enriched_sources = enriched_sources[:request.max_sources]                        # Update sources with relevance scores            final_sources = [                {                    "title": s["title"],                    "url": s["url"],                    "snippet": s["snippet"],                    "source_engine": s.get("source_engine", "unknown"),                    "relevance_score": s.get("relevance_score"),                }                for s in enriched_sources            ]            yield self._sse_event("sources_ranked", final_sources)                        # === Step 4: Generate Answer ===            yield self._sse_event("status", {"message": "Generating answer...", "step": 4})            yield self._sse_event("answer_start", {})                        messages = [                {"role": "system", "content": SYSTEM_PROMPT},                {"role": "user", "content": build_search_prompt(                    query=request.query,                    sources=enriched_sources                )}            ]                        full_answer = ""            async for chunk in self.llm.generate_stream(messages):                full_answer += chunk                yield self._sse_event("answer_chunk", {"text": chunk})                        yield self._sse_event("answer_end", {})                        # === Step 5: Related Queries ===            try:                related = await self._generate_related_queries(request.query, full_answer[:500])                yield self._sse_event("related", {"queries": related})            except Exception as e:                logger.debug(f"Related queries generation failed: {e}")                        # === Done ===            elapsed = time.time() - start_time            yield self._sse_event("done", {                "search_time": round(elapsed, 2),                "model_used": self.settings.llm_model,                "sources_count": len(final_sources),            })                    except Exception as e:            logger.error(f"Pipeline error: {e}", exc_info=True)            yield self._sse_event("error", {"message": str(e)})        async def search_sync(self, request: SearchRequest) -> Dict[str, Any]:        """        Execute search pipeline synchronously (non-streaming).        Returns complete response.        """        start_time = time.time()                # Search        search_results = await self.aggregator.search(            query=request.query,            max_results=request.max_sources * 2        )                if not search_results:            return {"error": "No search results found"}                # Fetch content        urls = [s.url for s in search_results[:self.settings.content_max_pages]]        html_results = await self.fetcher.fetch_pages(urls)                enriched_sources = []        for source in search_results:            source_dict = source.model_dump()            html = html_results.get(source.url)            if html:                content = self.extractor.extract(html, source.url)                if content:                    source_dict["content"] = content            enriched_sources.append(source_dict)                # Rerank        if self.reranker.is_loaded:            enriched_sources = self.reranker.rerank(                query=request.query,                documents=enriched_sources,                top_k=request.max_sources            )        else:            enriched_sources = enriched_sources[:request.max_sources]                # Generate answer        messages = [            {"role": "system", "content": SYSTEM_PROMPT},            {"role": "user", "content": build_search_prompt(                query=request.query,                sources=enriched_sources            )}        ]                answer = await self.llm.generate(messages)                elapsed = time.time() - start_time                return {            "query": request.query,            "answer": answer,            "sources": [                {                    "title": s["title"],                    "url": s["url"],                    "snippet": s["snippet"],                    "relevance_score": s.get("relevance_score"),                }                for s in enriched_sources            ],            "search_time": round(elapsed, 2),            "model_used": self.settings.llm_model,        }        async def _generate_related_queries(self, query: str, answer_summary: str) -> List[str]:        """Generate related follow-up queries."""        messages = [            {"role": "user", "content": RELATED_QUERIES_PROMPT.format(                query=query,                answer_summary=answer_summary            )}        ]                response = await self.llm.generate(messages, max_tokens=200)                # Parse response into list        related = [            line.strip().lstrip("0123456789.-) ")             for line in response.strip().split("\n")             if line.strip() and len(line.strip()) > 5        ]                return related[:3]        def _sse_event(self, event: str, data: Any) -> str:        """Format a Server-Sent Event string."""        return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n" ``` #### `backend/main.py` - FastAPI å…¥å£ ```python """ AutoSearch AI - FastAPI Application Entry Point """ import logging from contextlib import asynccontextmanager from fastapi import FastAPI, Query from fastapi.middleware.cors import CORSMiddleware from fastapi.responses import StreamingResponse, JSONResponse from fastapi.staticfiles import StaticFiles from backend.config import get_settings from backend.models.schemas import SearchRequest, HealthResponse from backend.pipeline.search_pipeline import SearchPipeline from backend.models.reranker import get_reranker from backend.llm.client import LLMClient # Configure logging logging.basicConfig(    level=logging.INFO,    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s" ) logger = logging.getLogger(__name__) # Global instances pipeline: SearchPipeline = None  @asynccontextmanager async def lifespan(app: FastAPI):    """Application lifespan - startup and shutdown."""    global pipeline        settings = get_settings()    logger.info(f"Starting {settings.app_name} v{settings.app_version}")    logger.info(f"LLM Provider: {settings.llm_provider}, Model: {settings.llm_model}")    logger.info(f"Search Engines: {settings.search_engines}")    logger.info(f"Reranker Enabled: {settings.reranker_enabled}")        # Initialize pipeline    pipeline = SearchPipeline()        # Pre-load reranker    reranker = get_reranker()    if reranker.is_loaded:        logger.info("Reranker model loaded and ready")        yield        logger.info("Shutting down AutoSearch AI")  # Create FastAPI app settings = get_settings() app = FastAPI(    title=settings.app_name,    version=settings.app_version,    description="AI-powered real-time search engine with source citations",    lifespan=lifespan, ) # CORS app.add_middleware(    CORSMiddleware,    allow_origins=settings.cors_origins + ["*"],  # Allow all in dev    allow_credentials=True,    allow_methods=["*"],    allow_headers=["*"], )  @app.get("/api/health", response_model=HealthResponse) async def health_check():    """Check application health status."""    settings = get_settings()    reranker = get_reranker()    llm = LLMClient()        return HealthResponse(        status="healthy",        version=settings.app_version,        llm_connected=await llm.check_connection(),        reranker_loaded=reranker.is_loaded,        search_engines=settings.search_engines,    )  @app.post("/api/search") async def search(request: SearchRequest):    """    Perform an AI-powered search.        - **stream=true** (default): Returns Server-Sent Events stream    - **stream=false**: Returns complete JSON response    """    global pipeline        if request.stream:        return StreamingResponse(            pipeline.search_stream(request),            media_type="text/event-stream",            headers={                "Cache-Control": "no-cache",                "Connection": "keep-alive",                "X-Accel-Buffering": "no",            }        )    else:        result = await pipeline.search_sync(request)        return JSONResponse(content=result)  @app.get("/api/search") async def search_get(    q: str = Query(..., description="Search query"),    mode: str = Query("quick", description="Search mode: quick, deep, academic"),    stream: bool = Query(True, description="Enable streaming"), ):    """GET endpoint for quick searches."""    request = SearchRequest(query=q, mode=mode, stream=stream)        if stream:        return StreamingResponse(            pipeline.search_stream(request),            media_type="text/event-stream",        )    else:        result = await pipeline.search_sync(request)        return JSONResponse(content=result)  # Serve frontend static files (production) import os frontend_dist = os.path.join(os.path.dirname(__file__), "..", "frontend", "out") if os.path.exists(frontend_dist):    app.mount("/", StaticFiles(directory=frontend_dist, html=True), name="frontend")  if __name__ == "__main__":    import uvicorn    uvicorn.run(        "backend.main:app",        host=settings.host,        port=settings.port,        reload=settings.debug,    ) ``` ### 3.3 å‰ç«¯æ ¸å¿ƒä»£ç  #### `frontend/src/app/page.tsx` - ä¸»é¡µ ```tsx "use client"; import { useState, useRef, useEffect } from "react"; import { SearchBar } from "@/components/SearchBar"; import { SourceCard } from "@/components/SourceCard"; import { AnswerPanel } from "@/components/AnswerPanel"; import { Header } from "@/components/Header"; import { LoadingAnimation } from "@/components/LoadingAnimation"; import { useSearch } from "@/hooks/useSearch"; export default function Home() {  const {     search,     isSearching,     sources,     answer,     status,     relatedQueries,     searchTime,    error   } = useSearch();    const [hasSearched, setHasSearched] = useState(false);   const handleSearch = async (query: string) => {    setHasSearched(true);    await search(query);  };   return (    <main className="min-h-screen bg-gradient-to-b from-gray-50 to-white dark:from-gray-950 dark:to-gray-900">      <Header />            <div className="max-w-4xl mx-auto px-4 py-8">        {/* Hero Section - shown before first search */}        {!hasSearched && (          <div className="text-center py-20">            <h1 className="text-5xl font-bold bg-gradient-to-r from-blue-600 to-purple-600 bg-clip-text text-transparent mb-4">              AutoSearch AI            </h1>            <p className="text-xl text-gray-600 dark:text-gray-400 mb-8">              AI-powered search with real-time web results and source citations            </p>          </div>        )}                {/* Search Bar */}        <SearchBar onSearch={handleSearch} isSearching={isSearching} />                {/* Status */}        {isSearching && status && (          <div className="mt-6">            <LoadingAnimation status={status} />          </div>        )}                {/* Error */}        {error && (          <div className="mt-6 p-4 bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-800 rounded-lg">            <p className="text-red-600 dark:text-red-400">{error}</p>          </div>        )}                {/* Sources */}        {sources.length > 0 && (          <div className="mt-6">            <h3 className="text-sm font-semibold text-gray-500 dark:text-gray-400 mb-3 flex items-center gap-2">              <svg className="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />              </svg>              SOURCES ({sources.length})            </h3>            <div className="flex gap-3 overflow-x-auto pb-2">              {sources.map((source, index) => (                <SourceCard key={source.url} source={source} index={index + 1} />              ))}            </div>          </div>        )}                {/* Answer */}        {(answer || isSearching) && hasSearched && (          <div className="mt-6">            <AnswerPanel               answer={answer}               isStreaming={isSearching}               sources={sources}            />          </div>        )}                {/* Search Time */}        {searchTime && !isSearching && (          <p className="mt-4 text-sm text-gray-400">            Search completed in {searchTime}s          </p>        )}                {/* Related Queries */}        {relatedQueries.length > 0 && !isSearching && (          <div className="mt-6">            <h3 className="text-sm font-semibold text-gray-500 dark:text-gray-400 mb-3">              Related Searches            </h3>            <div className="flex flex-wrap gap-2">              {relatedQueries.map((query, index) => (                <button                  key={index}                  onClick={() => handleSearch(query)}                  className="px-4 py-2 bg-gray-100 dark:bg-gray-800 hover:bg-gray-200 dark:hover:bg-gray-700 rounded-full text-sm text-gray-700 dark:text-gray-300 transition-colors"                >                  {query}                </button>              ))}            </div>          </div>        )}      </div>    </main>  ); } ``` #### `frontend/src/hooks/useSearch.ts` - æœç´¢ Hook ```typescript "use client"; import { useState, useCallback, useRef } from "react"; interface Source {  title: string;  url: string;  snippet: string;  source_engine: string;  relevance_score?: number; } interface SearchState {  isSearching: boolean;  sources: Source[];  answer: string;  status: string;  relatedQueries: string[];  searchTime: number | null;  error: string | null; } const API_BASE = process.env.NEXT_PUBLIC_API_URL || "http://localhost:8000"; export function useSearch() {  const [state, setState] = useState<SearchState>({    isSearching: false,    sources: [],    answer: "",    status: "",    relatedQueries: [],    searchTime: null,    error: null,  });    const abortControllerRef = useRef<AbortController | null>(null);   const search = useCallback(async (query: string) => {    // Cancel previous search    if (abortControllerRef.current) {      abortControllerRef.current.abort();    }        const controller = new AbortController();    abortControllerRef.current = controller;        setState({      isSearching: true,      sources: [],      answer: "",      status: "Starting search...",      relatedQueries: [],      searchTime: null,      error: null,    });     try {      const response = await fetch(`${API_BASE}/api/search`, {        method: "POST",        headers: { "Content-Type": "application/json" },        body: JSON.stringify({          query,          mode: "quick",          max_sources: 6,          stream: true,        }),        signal: controller.signal,      });       if (!response.ok) {        throw new Error(`HTTP error! status: ${response.status}`);      }       const reader = response.body?.getReader();      const decoder = new TextDecoder();       if (!reader) {        throw new Error("No response body");      }       let buffer = "";       while (true) {        const { done, value } = await reader.read();        if (done) break;         buffer += decoder.decode(value, { stream: true });        const lines = buffer.split("\n");        buffer = lines.pop() || "";         let currentEvent = "";                for (const line of lines) {          if (line.startsWith("event: ")) {            currentEvent = line.slice(7).trim();          } else if (line.startsWith("data: ") && currentEvent) {            try {              const data = JSON.parse(line.slice(6));                            switch (currentEvent) {                case "status":                  setState(prev => ({ ...prev, status: data.message }));                  break;                                  case "sources":                  setState(prev => ({ ...prev, sources: data }));                  break;                                  case "sources_ranked":                  setState(prev => ({ ...prev, sources: data }));                  break;                                  case "answer_start":                  setState(prev => ({ ...prev, status: "Generating answer..." }));                  break;                                  case "answer_chunk":                  setState(prev => ({                    ...prev,                    answer: prev.answer + data.text,                  }));                  break;                                  case "answer_end":                  break;                                  case "related":                  setState(prev => ({                    ...prev,                    relatedQueries: data.queries || [],                  }));                  break;                                  case "done":                  setState(prev => ({                    ...prev,                    isSearching: false,                    searchTime: data.search_time,                    status: "",                  }));                  break;                                  case "error":                  setState(prev => ({                    ...prev,                    isSearching: false,                    error: data.message,                    status: "",                  }));                  break;              }                            currentEvent = "";            } catch (e) {              // Ignore JSON parse errors for incomplete data            }          }        }      }    } catch (error: any) {      if (error.name === "AbortError") return;            setState(prev => ({        ...prev,        isSearching: false,        error: error.message || "Search failed",        status: "",      }));    }  }, []);   return {    search,    ...state,  }; } ``` #### `frontend/src/components/SearchBar.tsx` ```tsx "use client"; import { useState, useRef, KeyboardEvent } from "react"; interface SearchBarProps {  onSearch: (query: string) => void;  isSearching: boolean; } export function SearchBar({ onSearch, isSearching }: SearchBarProps) {  const [query, setQuery] = useState("");  const inputRef = useRef<HTMLTextAreaElement>(null);   const handleSubmit = () => {    const trimmed = query.trim();    if (trimmed && !isSearching) {      onSearch(trimmed);    }  };   const handleKeyDown = (e: KeyboardEvent<HTMLTextAreaElement>) => {    if (e.key === "Enter" && !e.shiftKey) {      e.preventDefault();      handleSubmit();    }  };   return (    <div className="relative">      <div className="flex items-end gap-2 p-2 bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-2xl shadow-lg focus-within:ring-2 focus-within:ring-blue-500 focus-within:border-transparent transition-all">        <textarea          ref={inputRef}          value={query}          onChange={(e) => setQuery(e.target.value)}          onKeyDown={handleKeyDown}          placeholder="Ask anything... Search the web with AI"          className="flex-1 resize-none bg-transparent border-none outline-none p-3 text-gray-900 dark:text-gray-100 placeholder-gray-400 min-h-[48px] max-h-[200px]"          rows={1}          disabled={isSearching}        />                <button          onClick={handleSubmit}          disabled={isSearching || !query.trim()}          className="flex items-center justify-center w-12 h-12 bg-blue-600 hover:bg-blue-700 disabled:bg-gray-300 dark:disabled:bg-gray-600 text-white rounded-xl transition-colors flex-shrink-0"        >          {isSearching ? (            <svg className="animate-spin w-5 h-5" fill="none" viewBox="0 0 24 24">              <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4" />              <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z" />            </svg>          ) : (            <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M14 5l7 7m0 0l-7 7m7-7H3" />            </svg>          )}        </button>      </div>    </div>  ); } ``` #### `frontend/src/components/SourceCard.tsx` ```tsx interface Source {  title: string;  url: string;  snippet: string;  source_engine: string; } interface SourceCardProps {  source: Source;  index: number; } export function SourceCard({ source, index }: SourceCardProps) {  const domain = new URL(source.url).hostname.replace("www.", "");  const favicon = `https://www.google.com/s2/favicons?domain=${domain}&sz=32`;   return (    <a      href={source.url}      target="_blank"      rel="noopener noreferrer"      className="flex-shrink-0 w-64 p-3 bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-xl hover:border-blue-300 dark:hover:border-blue-600 hover:shadow-md transition-all group"    >      <div className="flex items-center gap-2 mb-2">        <span className="flex items-center justify-center w-5 h-5 bg-blue-100 dark:bg-blue-900 text-blue-600 dark:text-blue-400 rounded text-xs font-bold">          {index}        </span>        <img src={favicon} alt="" className="w-4 h-4 rounded" />        <span className="text-xs text-gray-500 dark:text-gray-400 truncate">          {domain}        </span>      </div>            <h4 className="text-sm font-medium text-gray-900 dark:text-gray-100 line-clamp-2 group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors">        {source.title}      </h4>            <p className="mt-1 text-xs text-gray-500 dark:text-gray-400 line-clamp-2">        {source.snippet}      </p>    </a>  ); } ``` #### `frontend/src/components/AnswerPanel.tsx` ```tsx "use client"; import ReactMarkdown from "react-markdown"; import remarkGfm from "remark-gfm"; interface Source {  title: string;  url: string; } interface AnswerPanelProps {  answer: string;  isStreaming: boolean;  sources: Source[]; } export function AnswerPanel({ answer, isStreaming, sources }: AnswerPanelProps) {  // Replace [1], [2] etc. with clickable links  const processAnswer = (text: string) => {    return text.replace(/\[(\d+)\]/g, (match, num) => {      const index = parseInt(num) - 1;      if (index >= 0 && index < sources.length) {        return `[${num}](${sources[index].url})`;      }      return match;    });  };   return (    <div className="bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-xl p-6">      <div className="flex items-center gap-2 mb-4">        <div className="w-6 h-6 bg-gradient-to-r from-blue-500 to-purple-500 rounded-lg flex items-center justify-center">          <svg className="w-4 h-4 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 10V3L4 14h7v7l9-11h-7z" />          </svg>        </div>        <h3 className="font-semibold text-gray-900 dark:text-gray-100">Answer</h3>        {isStreaming && (          <span className="flex items-center gap-1 text-xs text-blue-500">            <span className="w-2 h-2 bg-blue-500 rounded-full animate-pulse" />            Generating...          </span>        )}      </div>            <div className="prose prose-gray dark:prose-invert max-w-none prose-sm prose-a:text-blue-600 dark:prose-a:text-blue-400 prose-a:no-underline hover:prose-a:underline">        <ReactMarkdown remarkPlugins={[remarkGfm]}>          {processAnswer(answer)}        </ReactMarkdown>        {isStreaming && (          <span className="inline-block w-2 h-5 bg-blue-500 animate-pulse ml-1" />        )}      </div>    </div>  ); } ``` ### 3.4 é…ç½®æ–‡ä»¶ #### `.env.example` ```bash # ============================================ # AutoSearch AI Configuration # Copy this file to .env and fill in your values # ============================================ # === LLM Provider === # Options: openai, ollama, groq, together, custom LLM_PROVIDER=openai LLM_API_KEY=sk-your-openai-api-key-here LLM_MODEL=gpt-4o-mini # LLM_BASE_URL=http://localhost:11434/v1    # For Ollama # LLM_MODEL=llama3.1                        # For Ollama # === Search Engines === # Comma-separated list: duckduckgo, google, bing, brave SEARCH_ENGINES=["duckduckgo"] # === Optional: Google Search API === # GOOGLE_API_KEY=your-google-api-key # GOOGLE_CX_ID=your-custom-search-engine-id # === Reranker === RERANKER_ENABLED=true # Set to false if you want faster startup without the small model # === App Settings === DEBUG=false HOST=0.0.0.0 PORT=8000 ``` #### `pyproject.toml` ```toml [build-system] requires = ["hatchling"] build-backend = "hatchling.build" [project] name = "autosearch-ai" version = "0.1.0" description = "AI-powered real-time search engine with source citations" readme = "README.md" license = {text = "MIT"} requires-python = ">=3.9" authors = [    {name = "Your Name", email = "your@email.com"}, ] keywords = ["ai", "search", "llm", "perplexity", "open-source"] classifiers = [    "Development Status :: 4 - Beta",    "Intended Audience :: Developers",    "License :: OSI Approved :: MIT License",    "Programming Language :: Python :: 3", ] dependencies = [    "fastapi>=0.104.0",    "uvicorn[standard]>=0.24.0",    "pydantic>=2.0",    "pydantic-settings>=2.0",    "openai>=1.10.0",    "aiohttp>=3.9.0",    "duckduckgo-search>=4.0",    "trafilatura>=1.6.0",    "beautifulsoup4>=4.12.0",    "lxml>=4.9.0", ] [project.optional-dependencies] reranker = [    "sentence-transformers>=2.2.0",    "torch>=2.0.0", ] dev = [    "pytest>=7.0",    "pytest-asyncio>=0.21",    "httpx>=0.25",    "ruff>=0.1.0", ] all = [    "autosearch-ai[reranker,dev]", ] [project.scripts] autosearch = "backend.main:app" [project.urls] Homepage = "https://github.com/yourusername/autosearch-ai" Documentation = "https://github.com/yourusername/autosearch-ai/docs" Repository = "https://github.com/yourusername/autosearch-ai" [tool.ruff] line-length = 100 target-version = "py39" ``` #### `docker-compose.yml` ```yaml version: '3.8' services:  autosearch:    build: .    ports:      - "8000:8000"    env_file:      - .env    environment:      - HOST=0.0.0.0      - PORT=8000    volumes:      - model_cache:/root/.cache  # Cache reranker model    restart: unless-stopped      # Optional: Ollama for local LLM  ollama:    image: ollama/ollama:latest    ports:      - "11434:11434"    volumes:      - ollama_data:/root/.ollama    profiles:      - local-llm  # Only start with: docker compose --profile local-llm up volumes:  model_cache:  ollama_data: ``` #### `Dockerfile` ```dockerfile # === Build Frontend === FROM node:20-alpine AS frontend-builder WORKDIR /app/frontend COPY frontend/package*.json ./ RUN npm ci COPY frontend/ ./ RUN npm run build # === Python Backend === FROM python:3.11-slim WORKDIR /app # Install system dependencies RUN apt-get update && apt-get install -y --no-install-recommends \    build-essential \    && rm -rf /var/lib/apt/lists/* # Install Python dependencies COPY pyproject.toml ./ RUN pip install --no-cache-dir ".[reranker]" # Copy backend code COPY backend/ ./backend/ # Copy built frontend COPY --from=frontend-builder /app/frontend/out ./frontend/out # Expose port EXPOSE 8000 # Run CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"] ``` --- ## ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´ README.md ```markdown <div align="center">   # ðŸ” AutoSearch AI ### Your AI-Powered Real-Time Search Engine [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/) [![GitHub stars](https://img.shields.io/github/stars/yourusername/autosearch-ai?style=social)](https://github.com/yourusername/autosearch-ai) An open-source, self-hosted AI search engine that combines real-time web search with LLM-powered answers and source citations. Think of it as your own Perplexity AI â€” free, private, and customizable. [Demo](#demo) Â· [Quick Start](#-quick-start) Â· [Features](#-features) Â· [API Docs](#-api) Â· [Contributing](#-contributing) ![AutoSearch AI Demo](docs/demo.gif) </div> --- ## âœ¨ Features - ðŸš€ **One-command setup** â€” Get started in under 2 minutes - ðŸ”„ **Multi-engine search** â€” DuckDuckGo (free, default), Google, Bing, Brave - ðŸ§  **Any LLM** â€” OpenAI, Ollama (local), Groq, Together, or any OpenAI-compatible API - ðŸ“ **Source citations** â€” Every claim is backed by clickable references [1][2][3] - ðŸŽ¯ **Smart reranker** â€” Optional 80MB model that dramatically improves result quality - âš¡ **Streaming responses** â€” See answers generate in real-time - ðŸŒ™ **Beautiful UI** â€” Modern, responsive design with dark mode - ðŸ”Œ **API-first** â€” Full REST API for integration into your projects - ðŸ³ **Docker ready** â€” One-click deployment with Docker Compose - ðŸ”’ **Privacy-first** â€” Self-hosted, your data never leaves your server ## ðŸš€ Quick Start ### Option 1: pip install (Recommended) ```bash # Install pip install autosearch-ai # Configure (minimum: just set your LLM API key) export LLM_API_KEY=sk-your-openai-key # Run python -m uvicorn backend.main:app --port 8000 ``` ### Option 2: From source ```bash # Clone git clone https://github.com/yourusername/autosearch-ai.git cd autosearch-ai # Install backend pip install -e ".[all]" # Configure cp .env.example .env # Edit .env with your API keys # Start backend python -m uvicorn backend.main:app --reload --port 8000 # Start frontend (in another terminal) cd frontend npm install npm run dev ``` ### Option 3: Docker ```bash git clone https://github.com/yourusername/autosearch-ai.git cd autosearch-ai cp .env.example .env # Edit .env docker compose up -d ``` ### Option 4: Use with Ollama (fully local, no API key needed!) ```bash # Start Ollama ollama pull llama3.1 # Configure AutoSearch AI export LLM_PROVIDER=ollama export LLM_MODEL=llama3.1 export LLM_BASE_URL=http://localhost:11434/v1 export RERANKER_ENABLED=true # Run python -m uvicorn backend.main:app --port 8000 ``` Open http://localhost:8000 and start searching! ðŸŽ‰ ## ðŸ—ï¸ Architecture ``` User Query â†’ Query Analysis â†’ Multi-Engine Search â†’ Content Extraction    â†’ Reranking (optional small model) â†’ LLM Synthesis â†’ Cited Answer ``` See [Architecture Documentation](docs/architecture.md) for details. ## ðŸ”Œ API ### Search Endpoint ```bash # Streaming (default) curl -X POST http://localhost:8000/api/search \  -H "Content-Type: application/json" \  -d '{"query": "latest AI news 2024", "stream": true}' # Non-streaming curl -X POST http://localhost:8000/api/search \  -H "Content-Type: application/json" \  -d '{"query": "latest AI news 2024", "stream": false}' ``` ### Response Format (non-streaming) ```json {  "query": "latest AI news 2024",  "answer": "Based on recent developments...[1]...[2]...",  "sources": [    {      "title": "AI Breakthroughs in 2024",      "url": "https://example.com/ai-news",      "snippet": "...",      "relevance_score": 0.95    }  ],  "search_time": 3.2,  "model_used": "gpt-4o-mini" } ``` ## âš™ï¸ Configuration | Variable | Default | Description | |----------|---------|-------------| | `LLM_PROVIDER` | `openai` | LLM provider: openai, ollama, groq, together, custom | | `LLM_API_KEY` | â€” | API key for the LLM provider | | `LLM_MODEL` | `gpt-4o-mini` | Model name | | `SEARCH_ENGINES` | `["duckduckgo"]` | Search engines to use | | `RERANKER_ENABLED` | `true` | Enable the 80MB reranker model | | `CONTENT_MAX_PAGES` | `6` | Max pages to fetch content from | See [.env.example](.env.example) for all options. ## ðŸ¤ Contributing We welcome contributions! See [CONTRIBUTING.md](docs/contributing.md). ## ðŸ“„ License MIT License - see [LICENSE](LICENSE). ## â­ Star History If you find this useful, please star the repo! It helps others discover the project. ``` --- ## ç¬¬äº”éƒ¨åˆ†ï¼šçˆ†æ¬¾ç­–ç•¥ä¸ŽæŽ¨å¹¿è®¡åˆ’ ### 5.1 å¼€æºé¡¹ç›®çš„ã€Œçˆ†æ¬¾è¦ç´ ã€æ¸…å• | è¦ç´  | å…·ä½“åšæ³• | é‡è¦åº¦ | |------|---------|--------| | **è§£å†³çœŸå®žç—›ç‚¹** | å…è´¹çš„ Perplexity æ›¿ä»£å“ | â­â­â­â­â­ | | **æžä½Žä¸Šæ‰‹é—¨æ§›** | 3 æ­¥ Quick Startï¼Œæ”¯æŒ `pip install` | â­â­â­â­â­ | | **ç²¾ç¾Ž README** | GIF æ¼”ç¤ºã€æ¸…æ™°è¡¨æ ¼ã€badge å¾½ç«  | â­â­â­â­â­ | | **æ¼”ç¤º GIF/è§†é¢‘** | 30 ç§’å½•å±å±•ç¤ºå®Œæ•´æœç´¢è¿‡ç¨‹ | â­â­â­â­â­ | | **å¤šç§éƒ¨ç½²æ–¹å¼** | pip / Docker / æºç  / Ollama æœ¬åœ° | â­â­â­â­ | | **API ä¼˜å…ˆ** | å¼€å‘è€…å¯ä»¥é›†æˆåˆ°è‡ªå·±é¡¹ç›® | â­â­â­â­ | | **æ´»è·ƒç»´æŠ¤** | åŠæ—¶å›žå¤ Issueï¼Œå®šæœŸæ›´æ–° | â­â­â­â­ | | **ç¤¾åŒºè®¨è®º** | GitHub Discussions å¼€å¯ | â­â­â­ | ### 5.2 æŽ¨å¹¿æ¸ é“ä¸Žæ—¶é—´çº¿ **Week 1: å‡†å¤‡é˜¶æ®µ** - [ ] å®Œæˆæ ¸å¿ƒä»£ç å¼€å‘ä¸Žæµ‹è¯• - [ ] å½•åˆ¶é«˜è´¨é‡ Demo GIFï¼ˆä½¿ç”¨ LICEcap/ScreenToGifï¼‰ - [ ] æ’°å†™å®Œæ•´ README å’Œæ–‡æ¡£ - [ ] åˆ›å»º Logoï¼ˆä½¿ç”¨ Figma æˆ–æ‰¾è®¾è®¡å¸ˆï¼‰ **Week 2: é¦–å‘æŽ¨å¹¿** - [ ] **Hacker News** â€” å‘å¸ƒ "Show HN: AutoSearch AI â€“ Open-source Perplexity alternative" - [ ] **Reddit** â€” r/selfhosted, r/opensource, r/artificial, r/LocalLLaMA - [ ] **Twitter/X** â€” é•¿æŽ¨æ–‡ä»‹ç» + Demo è§†é¢‘ï¼Œ@ç›¸å…³æŠ€æœ¯å¤§V - [ ] **Product Hunt** â€” å‡†å¤‡ Launchï¼ˆé€‰å‘¨äºŒåˆ°å‘¨å››ï¼‰ **Week 3: ä¸­æ–‡ç¤¾åŒº** - [ ] **V2EX** â€” å‘å¸ƒåˆ° "åˆ†äº«åˆ›é€ " èŠ‚ç‚¹ - [ ] **æŽ˜é‡‘** â€” æ’°å†™æŠ€æœ¯æ–‡ç«  - [ ] **çŸ¥ä¹Ž** â€” å›žç­” "æœ‰å“ªäº›å¥½ç”¨çš„å¼€æºæœç´¢å·¥å…·" ç±»é—®é¢˜ - [ ] **å³åˆ»** â€” å‘å¸ƒåˆ° AI ç›¸å…³åœˆå­ - [ ] **å¾®ä¿¡å…¬ä¼—å· / æŠ€æœ¯ç¤¾åŒº** â€” æŠ•ç¨¿ **Week 4+: æŒç»­è¿è¥** - [ ] æ¯ 2 å‘¨å‘å¸ƒä¸€ä¸ªæ–°ç‰ˆæœ¬ - [ ] å¾é›† Feature Request - [ ] å†™æŠ€æœ¯åšå®¢è§£é‡Šæž¶æž„è®¾è®¡ - [ ] åˆ¶ä½œ YouTube/Bç«™ æ•™ç¨‹è§†é¢‘ ### 5.3 æ‰“é€ ã€Œç¤¾äº¤è¯æ˜Žã€çš„æŠ€å·§ 1. **Star æ›²çº¿å¯åŠ¨**: æ‰¾ 10-20 ä¸ªæœ‹å‹/åŒäº‹å…ˆ Starï¼Œå½¢æˆåˆå§‹åŠ¨åŠ› 2. **Issue æ¨¡æ¿**: é¢„åˆ›å»º 3-5 ä¸ª Enhancement Issueï¼Œè®©é¡¹ç›®çœ‹èµ·æ¥æ´»è·ƒ 3. **Release è®°å½•**: å³ä½¿å°æ”¹åŠ¨ä¹Ÿå‘ Releaseï¼ŒGitHub ä¼šæŽ¨é€ç»™ Watchers 4. **Sponsor æŒ‰é’®**: å¼€å¯ GitHub Sponsorsï¼Œè¡¨æ˜Žä½ æ˜¯è®¤çœŸç»´æŠ¤çš„ 5. **Awesome åˆ—è¡¨**: æäº¤åˆ° awesome-selfhostedã€awesome-ai ç­‰åˆ—è¡¨ ### 5.4 åŽç»­åŠŸèƒ½è§„åˆ’ï¼ˆRoadmapï¼‰ åœ¨ README ä¸­å±•ç¤º Roadmap èƒ½è®©ç”¨æˆ·çŸ¥é“é¡¹ç›®æœ‰é•¿æœŸè§„åˆ’ï¼š ``` ## ðŸ—ºï¸ Roadmap - [x] Core search pipeline - [x] Multi-engine search aggregation - [x] Streaming responses with SSE - [x] Reranker small model - [ ] ðŸ”„ Conversation history / Follow-up questions - [ ] ðŸ“Š Knowledge graph visualization - [ ] ðŸ” Image search support - [ ] ðŸ“± PWA / Mobile app - [ ] ðŸŒ Multi-language UI - [ ] ðŸ”— Browser extension - [ ] ðŸ“¥ Export answers to Markdown/PDF - [ ] ðŸ¤– Agent mode (multi-step reasoning) - [ ] ðŸ” Authentication & user management - [ ] â˜ï¸ One-click deploy to Vercel/Railway/Fly.io ``` --- ## ç¬¬å…­éƒ¨åˆ†ï¼šå…³é”®æŠ€æœ¯ç»†èŠ‚ä¸Žä¼˜åŒ–å»ºè®® ### 6.1 Reranker æ¨¡åž‹é€‰æ‹©ç†ç”± | æ¨¡åž‹ | å¤§å° | è´¨é‡ | é€Ÿåº¦(CPU) | |------|------|------|-----------| | `cross-encoder/ms-marco-TinyBERT-L-2` | ~17MB | ä¸­ç­‰ | æžå¿« | | **`cross-encoder/ms-marco-MiniLM-L-6-v2`** | **~80MB** | **é«˜** | **å¿«** | | `cross-encoder/ms-marco-MiniLM-L-12-v2` | ~130MB | å¾ˆé«˜ | ä¸­ç­‰ | | `BAAI/bge-reranker-base` | ~1.1GB | æžé«˜ | æ…¢ | é€‰æ‹© MiniLM-L-6-v2 æ˜¯å› ä¸ºå®ƒåœ¨è´¨é‡å’Œå¤§å°ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚80MB çš„æ¨¡åž‹å®Œå…¨å¯ä»¥åœ¨ CPU ä¸Šå¿«é€ŸæŽ¨ç†ï¼Œå¯¹æœç´¢ç»“æžœçš„æ”¹å–„æ•ˆæžœæ˜¾è‘—ã€‚ ### 6.2 æœç´¢ç»“æžœè´¨é‡ä¼˜åŒ–ç­–ç•¥ ```python # åœ¨ pipeline ä¸­å®žæ–½çš„ä¼˜åŒ–ç­–ç•¥ï¼š # 1. å¤šå¼•æ“Žäº¤å‰éªŒè¯ # å¦‚æžœåŒä¸€æ¡ç»“æžœè¢«å¤šä¸ªå¼•æ“Žè¿”å›žï¼Œæå‡å…¶æƒé‡ # 2. åŸŸåè´¨é‡è¿‡æ»¤ QUALITY_DOMAINS = {"wikipedia.org", "arxiv.org", "github.com", "stackoverflow.com"} LOW_QUALITY_DOMAINS = {"pinterest.com", "quora.com"} # 3. æ—¶æ•ˆæ€§åŠ æƒ # è¿‘æœŸå‘å¸ƒçš„å†…å®¹èŽ·å¾—æ›´é«˜æƒé‡ # 4. å†…å®¹é•¿åº¦è¿‡æ»¤ # è¿‡çŸ­çš„é¡µé¢ï¼ˆ<100å­—ï¼‰å¯èƒ½æ˜¯ä½Žè´¨é‡å†…å®¹ ``` ### 6.3 æ€§èƒ½ä¼˜åŒ– 1. **å¹¶å‘æŠ“å–**: ä½¿ç”¨ `asyncio.gather` å¹¶å‘è¯·æ±‚å¤šä¸ªæœç´¢å¼•æ“Žå’Œç½‘é¡µ 2. **ç®€å•ç¼“å­˜**: ç›¸åŒæŸ¥è¯¢åœ¨ TTL å†…è¿”å›žç¼“å­˜ç»“æžœ 3. **æµå¼å“åº”**: SSE è®©ç”¨æˆ·æ„ŸçŸ¥åˆ°çš„å»¶è¿Ÿæœ€å°åŒ– 4. **æ‡’åŠ è½½ Reranker**: æ¨¡åž‹ä»…åœ¨é¦–æ¬¡éœ€è¦æ—¶åŠ è½½ 5. **è¿žæŽ¥æ± **: aiohttp å¤ç”¨ TCP è¿žæŽ¥ ### 6.4 é”™è¯¯å¤„ç†æœ€ä½³å®žè·µ ```python # å…³é”®åŽŸåˆ™ï¼šä¼˜é›…é™çº§ï¼Œè€Œéžå®Œå…¨å¤±è´¥ # æœç´¢å¼•æ“Žå¤±è´¥ â†’ ä½¿ç”¨å…¶ä»–å¼•æ“Žçš„ç»“æžœ # ç½‘é¡µæŠ“å–å¤±è´¥ â†’ ä½¿ç”¨æœç´¢æ‘˜è¦ä»£æ›¿æ­£æ–‡ # Reranker åŠ è½½å¤±è´¥ â†’ è·³è¿‡é‡æŽ’åºï¼Œä½¿ç”¨åŽŸå§‹æŽ’åº # LLM å¤±è´¥ â†’ è¿”å›žæœç´¢ç»“æžœåˆ—è¡¨ï¼ˆæ— AIæ€»ç»“ï¼‰ ``` --- ## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæµ‹è¯•ä¸Žè´¨é‡ä¿è¯ ### `tests/test_pipeline.py` ```python """ Integration tests for the search pipeline. """ import pytest import asyncio from unittest.mock import AsyncMock, patch from backend.pipeline.search_pipeline import SearchPipeline from backend.models.schemas import SearchRequest  @pytest.fixture def pipeline():    return SearchPipeline()  @pytest.mark.asyncio async def test_search_returns_results(pipeline):    """Test that search returns valid results."""    request = SearchRequest(        query="Python programming language",        mode="quick",        max_sources=3,        stream=False,    )        result = await pipeline.search_sync(request)        assert "answer" in result    assert "sources" in result    assert len(result["sources"]) > 0    assert result["search_time"] > 0  @pytest.mark.asyncio async def test_streaming_search(pipeline):    """Test that streaming search yields valid events."""    request = SearchRequest(        query="What is FastAPI?",        mode="quick",        max_sources=3,        stream=True,    )        events = []    async for event in pipeline.search_stream(request):        events.append(event)        # Should have at minimum: status, sources, answer chunks, done    assert len(events) >= 4        # Check for required event types    event_types = set()    for event in events:        if event.startswith("event: "):            event_types.add(event.split("\n")[0].replace("event: ", ""))        assert "sources" in event_types    assert "answer_chunk" in event_types or "answer_start" in event_types  @pytest.mark.asyncio   async def test_empty_query_handling():    """Test that empty queries are handled."""    with pytest.raises(Exception):        request = SearchRequest(query="", mode="quick") ``` ### GitHub Actions CI: `.github/workflows/ci.yml` ```yaml name: CI on:  push:    branches: [main]  pull_request:    branches: [main] jobs:  test:    runs-on: ubuntu-latest    strategy:      matrix:        python-version: ["3.9", "3.10", "3.11", "3.12"]        steps:      - uses: actions/checkout@v4            - name: Set up Python ${{ matrix.python-version }}        uses: actions/setup-python@v5        with:          python-version: ${{ matrix.python-version }}            - name: Install dependencies        run: |          pip install -e ".[dev]"            - name: Lint with ruff        run: ruff check backend/            - name: Run tests        run: pytest tests/ -v --tb=short        env:          LLM_PROVIDER: "custom"          LLM_BASE_URL: "http://localhost:11434/v1"          RERANKER_ENABLED: "false"   frontend:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v4      - uses: actions/setup-node@v4        with:          node-version: 20      - run: cd frontend && npm ci      - run: cd frontend && npm run build ``` --- ## ç¬¬å…«éƒ¨åˆ†ï¼šå¼€å‘æ‰§è¡Œè®¡åˆ’ ### 8.1 åˆ†é˜¶æ®µå¼€å‘è®¡åˆ’ **Phase 1 (3-5å¤©): MVP** - [x] åŽç«¯æœç´¢ç®¡çº¿ï¼ˆDuckDuckGo + å†…å®¹æå– + LLMï¼‰ - [x] åŸºæœ¬ API ç«¯ç‚¹ - [x] ç®€å•çš„å‰ç«¯ç•Œé¢ - [x] Docker éƒ¨ç½² **Phase 2 (3-5å¤©): å¢žå¼º** - [x] Reranker é›†æˆ - [x] æµå¼å“åº” - [x] å¤šæœç´¢å¼•æ“Ž - [x] ç²¾ç¾Ž UI - [x] å®Œæ•´ README **Phase 3 (2-3å¤©): å‘å¸ƒå‡†å¤‡** - [ ] æµ‹è¯•è¦†ç›– - [ ] CI/CD - [ ] Demo GIF å½•åˆ¶ - [ ] æŽ¨å¹¿æ–‡æ¡ˆå‡†å¤‡ - [ ] PyPI å‘å¸ƒ **Phase 4 (æŒç»­): ç¤¾åŒºè¿è¥** - [ ] å“åº” Issues - [ ] åˆå¹¶ PR - [ ] å®šæœŸå‘å¸ƒæ–°ç‰ˆæœ¬ - [ ] æ’°å†™æŠ€æœ¯åšå®¢ ### 8.2 æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥ ```bash # å¼€å‘ pip install -e ".[all]"                    # å®‰è£…æ‰€æœ‰ä¾èµ– uvicorn backend.main:app --reload          # å¯åŠ¨åŽç«¯ï¼ˆçƒ­é‡è½½ï¼‰ cd frontend && npm run dev                 # å¯åŠ¨å‰ç«¯ # æµ‹è¯• pytest tests/ -v                           # è¿è¡Œæµ‹è¯• ruff check backend/                        # ä»£ç æ£€æŸ¥ # éƒ¨ç½² docker compose up -d                       # Docker éƒ¨ç½² docker compose --profile local-llm up -d   # åŒ…å« Ollama # å‘å¸ƒ python -m build                            # æž„å»ºåŒ… twine upload dist/*                        # å‘å¸ƒåˆ° PyPI ``` --- ## æ€»ç»“ è¿™ä¸ªé¡¹ç›®ä¹‹æ‰€ä»¥æœ‰çˆ†æ¬¾æ½œåŠ›ï¼Œæ ¸å¿ƒåœ¨äºŽï¼š 1. **éœ€æ±‚æ˜Žç¡®** â€” Perplexity çš„å¼€æºæ›¿ä»£å“æ˜¯ä¸€ä¸ªè¢«éªŒè¯çš„éœ€æ±‚ 2. **é—¨æ§›æžä½Ž** â€” æ”¯æŒ pip install + ä¸€æ¡å‘½ä»¤å¯åŠ¨ 3. **çµæ´»æ€§é«˜** â€” æ”¯æŒæœ¬åœ°æ¨¡åž‹å’Œäº‘ç«¯ APIï¼Œç”¨æˆ·é€‰æ‹©æƒå¤§ 4. **å·®å¼‚åŒ–æ˜Žæ˜¾** â€” è½»é‡çº§ reranker æ˜¯äº®ç‚¹ï¼Œå…è´¹æœç´¢å¼•æ“Žé™ä½Žæˆæœ¬ 5. **å‰ç«¯ç²¾ç¾Ž** â€” çŽ°ä»£åŒ– UI æå‡ç¬¬ä¸€å°è±¡ 6. **æ–‡æ¡£å®Œå–„** â€” å®Œæ•´çš„ README + æž¶æž„æ–‡æ¡£ + API æ–‡æ¡£ **æœ€å…³é”®çš„æ‰§è¡Œå»ºè®®ï¼šå…ˆåšå‡ºèƒ½è·‘çš„ MVPï¼Œå½•åˆ¶ä¸€ä¸ªæƒŠè‰³çš„ Demo GIFï¼Œç„¶åŽåœ¨ Hacker News å’Œ Reddit ä¸Šé¦–å‘ã€‚ç¬¬ä¸€æ³¢æµé‡å†³å®šäº†é¡¹ç›®èƒ½å¦èµ·é£žã€‚**